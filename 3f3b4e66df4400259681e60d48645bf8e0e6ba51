{
  "comments": [
    {
      "key": {
        "uuid": "cd551897_3a8f6b6b",
        "filename": "rw_core/core/device_agent.go",
        "patchSetId": 2
      },
      "lineNbr": 200,
      "author": {
        "id": 1000056
      },
      "writtenOn": "2019-09-26T14:38:05Z",
      "side": 1,
      "message": "The reason the lock was kept on the device until an ACK is received from an Adapter is to prevent another request on the same device happening leading to potential data integrity problem.   Note that if the Adapter does not respond with an ACK there will be a timeout and this lock will be released.  Unless you are seeing an actual problem with this I will leave it as is.",
      "revId": "3f3b4e66df4400259681e60d48645bf8e0e6ba51",
      "serverId": "2a2bfe1b-c5c2-48ed-9ac8-16438ab24388",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "49b10792_52333d4d",
        "filename": "rw_core/core/device_agent.go",
        "patchSetId": 2
      },
      "lineNbr": 200,
      "author": {
        "id": 1000218
      },
      "writtenOn": "2019-09-26T18:06:35Z",
      "side": 1,
      "message": "By keeping lock, as you said, you prevent others to change the data, so adapter itself can not change the data, so why do we wait for adapter response for data integrity if it has no effect on data? And we also don\u0027t do anything to change or get the data after this point regardless of the response(ACK or NACK). If we did all about data, then I think we are done with the lock.\n\nAbout possible problems of keeping the lock for long, I wrote something as a response of David\u0027s comment.",
      "parentUuid": "cd551897_3a8f6b6b",
      "revId": "3f3b4e66df4400259681e60d48645bf8e0e6ba51",
      "serverId": "2a2bfe1b-c5c2-48ed-9ac8-16438ab24388",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "e81f1d06_3269f4e1",
        "filename": "rw_core/core/device_agent.go",
        "patchSetId": 2
      },
      "lineNbr": 200,
      "author": {
        "id": 1000056
      },
      "writtenOn": "2019-09-26T18:16:27Z",
      "side": 1,
      "message": "Mahir, you are assuming that a change to the data happens only from the adapter.  There are NB API that can change the data as well.",
      "parentUuid": "49b10792_52333d4d",
      "revId": "3f3b4e66df4400259681e60d48645bf8e0e6ba51",
      "serverId": "2a2bfe1b-c5c2-48ed-9ac8-16438ab24388",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "c9068766_e324abec",
        "filename": "rw_core/core/device_agent.go",
        "patchSetId": 2
      },
      "lineNbr": 200,
      "author": {
        "id": 1000218
      },
      "writtenOn": "2019-09-26T18:21:38Z",
      "side": 1,
      "message": "Sure, they can do. I mean by locking the Kafka messaging time, how can we keep data integrity. We already got the device, made our changes and commit it to DB. We don\u0027t have anything to do after Kafka response, so why waiting Kafka response.",
      "parentUuid": "e81f1d06_3269f4e1",
      "revId": "3f3b4e66df4400259681e60d48645bf8e0e6ba51",
      "serverId": "2a2bfe1b-c5c2-48ed-9ac8-16438ab24388",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "bcab8be3_b45634e9",
        "filename": "rw_core/core/device_agent.go",
        "patchSetId": 2
      },
      "lineNbr": 200,
      "author": {
        "id": 1000056
      },
      "writtenOn": "2019-09-26T18:34:02Z",
      "side": 1,
      "message": "A transaction in the Core starts when it receives a request and, in the case where the request will go to the adapter, the request is send to an adapter and an ACK is received.  This is the general pattern.   The lock is a safeguard for a transaction not fully being completed and another transaction start changing the data.   If I remember well there was a time openolt was not returning an ACK after an adopt_device and started changing data leading to incorrect behavior.  You were involved in that. We should have a lock for the duration of a transaction in the Core.",
      "parentUuid": "c9068766_e324abec",
      "revId": "3f3b4e66df4400259681e60d48645bf8e0e6ba51",
      "serverId": "2a2bfe1b-c5c2-48ed-9ac8-16438ab24388",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "f2f4a18e_a3ec8973",
        "filename": "rw_core/core/device_agent.go",
        "patchSetId": 2
      },
      "lineNbr": 200,
      "author": {
        "id": 1000218
      },
      "writtenOn": "2019-09-26T19:09:14Z",
      "side": 1,
      "message": "Yes Khen, I remember the case. In our system now, receiving a response from olt-adapter or Core, means that the message received by the appropriate container(because it sends the response immediately), but does not mean that the related data is changed or all the related things are done. So we prevent nothing by keeping lock during this Kafka messaging. In that previous case, we changed this behavior for a specific message and sent some info in the message.  \nI agree with the general approach but still think that the locks should cover the code blocks that we reached data to read or write and should immediately be released after data manipulation is done.",
      "parentUuid": "bcab8be3_b45634e9",
      "revId": "3f3b4e66df4400259681e60d48645bf8e0e6ba51",
      "serverId": "2a2bfe1b-c5c2-48ed-9ac8-16438ab24388",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "7764087c_b92ec4f6",
        "filename": "rw_core/core/device_agent.go",
        "patchSetId": 2
      },
      "lineNbr": 203,
      "author": {
        "id": 1000003
      },
      "writtenOn": "2019-09-26T14:29:47Z",
      "side": 1,
      "message": "By removing this out of the lock protection is there a risk that multiple threads attempt to adopt the device? What is the benefit of moving this out of lock protection or what is the harm by keeping it in protection?",
      "range": {
        "startLine": 203,
        "startChar": 5,
        "endLine": 203,
        "endChar": 23
      },
      "revId": "3f3b4e66df4400259681e60d48645bf8e0e6ba51",
      "serverId": "2a2bfe1b-c5c2-48ed-9ac8-16438ab24388",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "533712ae_f25b3416",
        "filename": "rw_core/core/device_agent.go",
        "patchSetId": 2
      },
      "lineNbr": 203,
      "author": {
        "id": 1000218
      },
      "writtenOn": "2019-09-26T18:06:35Z",
      "side": 1,
      "message": "The aim of lock is to prevent simultaneous read or writes to a data(here it is Device), and you generally want to get rid of it as soon as possible to prevent other threads wait for a long time on the same lock. Here after this point we do not get or update the data, so IMHO no need to keep the lock until the message sent from Kafka and the response received. There is also nothing done by the negative  response of the Kafka message as transaction rollback etc. So why do we wait for Kafka response to unlock the agent?\nThere is no change in behavior for the risk you mentioned David, because we already did the changes in data, and committed it to cache and KV store. Another thread should check the state in KV store, not Kafka message transfer success. As I said we don\u0027t make a change in data in NACK case.\n\nAnother risk for keeping locks too long is  changing the order of the access to data. We have upcoming statusUpdates and GetDevices from olt-adapter when this enableDevice operation ongoing. Because we are using mutexes, but not channels, if this lock takes too much time, then we probably receive more than one thread waiting for the same mutex(first an UpdateState, then a GetDevice). After the unlock there is no guarantee that the UpdateState thread will get the lock before GetDevice thread. This will fail your ONU activation then. By the way it may be beter to open a discussion to change the mutex allocation by golang channels.",
      "parentUuid": "7764087c_b92ec4f6",
      "range": {
        "startLine": 203,
        "startChar": 5,
        "endLine": 203,
        "endChar": 23
      },
      "revId": "3f3b4e66df4400259681e60d48645bf8e0e6ba51",
      "serverId": "2a2bfe1b-c5c2-48ed-9ac8-16438ab24388",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "4970ce5c_820f7b96",
        "filename": "rw_core/core/device_agent.go",
        "patchSetId": 2
      },
      "lineNbr": 203,
      "author": {
        "id": 1000056
      },
      "writtenOn": "2019-09-26T19:00:20Z",
      "side": 1,
      "message": "In your example of statusUpdates and getDevices from the adapter, since these messages come over kafka then kafka guarantees the order of these messages.   Also, using channels instead of mutex may theoretically be better but adds more complexity. \n\nNote that we started a transaction management discussion a few weeks ago (with rollbacks) on a TST call.  It got sidetrack to pursue Adapter HA.   Once that discussion gets resumed and the lifecycle of a transaction is agreed upon then we may or may not need to review the locking phases.",
      "parentUuid": "533712ae_f25b3416",
      "range": {
        "startLine": 203,
        "startChar": 5,
        "endLine": 203,
        "endChar": 23
      },
      "revId": "3f3b4e66df4400259681e60d48645bf8e0e6ba51",
      "serverId": "2a2bfe1b-c5c2-48ed-9ac8-16438ab24388",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "16a49667_9737703e",
        "filename": "rw_core/core/device_agent.go",
        "patchSetId": 2
      },
      "lineNbr": 203,
      "author": {
        "id": 1000218
      },
      "writtenOn": "2019-09-26T19:17:30Z",
      "side": 1,
      "message": "Kafka does its job and give it to Core in order, but core takes it from Kafka and executes them in different threads. It is not Kafka\u0027s responsibility any more. These two different threads are now waiting for same lockout the same time, which is acquired by eanbleDevice thread, to be released. \nIt makes sense  waiting the tr management discussion.",
      "parentUuid": "4970ce5c_820f7b96",
      "range": {
        "startLine": 203,
        "startChar": 5,
        "endLine": 203,
        "endChar": 23
      },
      "revId": "3f3b4e66df4400259681e60d48645bf8e0e6ba51",
      "serverId": "2a2bfe1b-c5c2-48ed-9ac8-16438ab24388",
      "unresolved": false
    }
  ]
}