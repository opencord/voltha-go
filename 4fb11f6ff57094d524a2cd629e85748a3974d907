{
  "comments": [
    {
      "key": {
        "uuid": "d4361de1_a7e82ea0",
        "filename": "python/ofagent/connection_mgr.py",
        "patchSetId": 22
      },
      "lineNbr": 250,
      "author": {
        "id": 1000056
      },
      "writtenOn": "2019-11-08T16:23:36Z",
      "side": 1,
      "message": "If I recall correctly the reason the ofagent was committing suicide was that there were situations where grpc goes into transient failure and never recuperate even after the far end comes back.  This was when ofagent was communicating to the cores directly instead via the afrouter.   I have not played with it for a while, I don\u0027t know if this is still the case.",
      "revId": "4fb11f6ff57094d524a2cd629e85748a3974d907",
      "serverId": "2a2bfe1b-c5c2-48ed-9ac8-16438ab24388",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "cebc6ed2_f6671cbd",
        "filename": "python/ofagent/connection_mgr.py",
        "patchSetId": 22
      },
      "lineNbr": 250,
      "author": {
        "id": 1000016
      },
      "writtenOn": "2019-11-08T17:02:35Z",
      "side": 1,
      "message": "Hmm, we\u0027re going to have to come up with a way to test for that scenario. I didn\u0027t observe any issues when I tried cycling the Kafka on and off to cause the core to go in and out of UNAVAILABLE state. We did fix some gRPC reliability issues a year or two back by upgrading to a newer version. Perhaps that resolved the issue here.\n\nRegardless, the suicide is broken. Ofagent would receive the signal and attempt to self-terminate, but it actually hangs, probably with some python thread left running keeping the process alive.\n\nNow that we have readiness and health probes, we should address restart via K8s, and let K8s restart the process if it is not healthy.",
      "parentUuid": "d4361de1_a7e82ea0",
      "revId": "4fb11f6ff57094d524a2cd629e85748a3974d907",
      "serverId": "2a2bfe1b-c5c2-48ed-9ac8-16438ab24388",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "3804288c_53aa4c3d",
        "filename": "rw_core/config/config.go",
        "patchSetId": 22
      },
      "lineNbr": 58,
      "author": {
        "id": 1000526
      },
      "writtenOn": "2019-11-08T17:46:14Z",
      "side": 1,
      "message": "After testing the liveness check added for ro_core today, I feel default Not Live Interval of 60 seconds is too high; should be around 5-10 seconds. Currently, k8s liveness probe for rw_core checks for POD health every 3 seconds and considers it down after 3 failures; and thus restarts the PoD. So, PoD has only around 10-12 seconds to recover from a failed kafka/kvstore connection before it will be bounced by k8s probe mechanism. PoD does not stand a chance to recover back with the current default NotLiveProbeInterval of 60 seconds.\n\nAlso, we should finetune the interval \u0026 max failures configured for Liveness and Readiness probes across the containers. Probably something to look at after implementing Probe logic across all the voltha modules.",
      "revId": "4fb11f6ff57094d524a2cd629e85748a3974d907",
      "serverId": "2a2bfe1b-c5c2-48ed-9ac8-16438ab24388",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "99c3451e_55dd0de7",
        "filename": "rw_core/config/config.go",
        "patchSetId": 22
      },
      "lineNbr": 58,
      "author": {
        "id": 1000107
      },
      "writtenOn": "2019-11-08T18:27:48Z",
      "side": 1,
      "message": "A pod should not be restarted if a dependency service is having issues.  (Not sure about this current implementation.  If you have observed such an issue, it needs to be adressed.)  \n\nThe ready probe should never trigger pod restart, and a dependency service\u0027s failure should only affect readiness, not liveness.\n\n(Can anyone confirm or deny these assumptions?)",
      "parentUuid": "3804288c_53aa4c3d",
      "revId": "4fb11f6ff57094d524a2cd629e85748a3974d907",
      "serverId": "2a2bfe1b-c5c2-48ed-9ac8-16438ab24388",
      "unresolved": false
    }
  ]
}